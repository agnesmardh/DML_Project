{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root for images\n",
    "dataroot = \"train\"\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size\n",
    "batch_size = 1\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 64\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 15\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "# Create the dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_real_gen(dataloader, model):\n",
    "    # Grab a batch of real images from the dataloader\n",
    "    real_batch = next(iter(dataloader))\n",
    "\n",
    "    # Plot the real images\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Real Images\")\n",
    "    plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "    # Plot the generated fake images\n",
    "    noise = torch.randn(real_batch.shape[0], 100, 1, 1, device=device)\n",
    "    fake_images = model.forward(noise)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Fake Images\")\n",
    "    plt.imshow(np.transpose(fake_images, (1,2,0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_real_image(dataloader):\n",
    "    real = next(iter(dataloader))\n",
    "    return real[0][0] \n",
    "    \n",
    "def create_fake_images(model, is_cond=False, batch_size=1):\n",
    "    noise = torch.randn(batch_size, 100, 1, 1, device=device)\n",
    "    \n",
    "    if is_cond:\n",
    "        fake_labels = torch.randint(0,2,(batch_size,)).to(device)\n",
    "        fake = model.forward(noise, fake_labels)\n",
    "        \n",
    "        return fake, fake_labels\n",
    "    else:\n",
    "        fake = model.forward(noise)\n",
    "        \n",
    "        return fake, None\n",
    "    \n",
    "\n",
    "def create_images_mixed(dataloader, model, is_cond=False):\n",
    "    real = create_real_image(dataloader)\n",
    "    \n",
    "    fake, fake_labels = create_fake_images(model, is_cond)\n",
    "    \n",
    "    return real, fake, fake_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(img, axes):\n",
    "    real_image_data = img.permute(1, 2, 0).numpy()\n",
    "    height, width, _ = real_image_data.shape\n",
    "    axes.imshow(real_image_data)\n",
    "    axes.set_xlim(0, width)\n",
    "    axes.set_ylim(height, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images_mixed(real, fake):\n",
    "    fig, axes = plt.subplots(1,2)\n",
    "    \n",
    "    real_index = random.randint(0, 1)\n",
    "    fake_index = 1 - real_index \n",
    "    \n",
    "    plot_image(real, axes[real_index])\n",
    "    \n",
    "    fake = fake[0].to(\"cpu\").detach()\n",
    "    plot_image(fake, axes[fake_index])\n",
    "    \n",
    "    axes[0].set_title(\"0\")\n",
    "    axes[1].set_title(\"1\")\n",
    "    \n",
    "    plt.show()\n",
    "    x = input(\"Continue...\")\n",
    "    # Clear output\n",
    "    return fake_index, x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_labeled_fake_image(fake, fake_label):\n",
    "    fig, axes = plt.subplots()\n",
    "    \n",
    "    fake.detach()\n",
    "    plot_image(fake, axes)\n",
    "    axes.set_title(\"cat = 0, dog = 1, unknown = -1\")\n",
    "    \n",
    "    plt.show()\n",
    "    x = input(\"Guess the label: \")\n",
    "    \n",
    "    return fake_label, x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnconditionalGenerator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(UnconditionalGenerator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGenerator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(ConditionalGenerator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        self.label_emb = nn.Embedding(2, 2)\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz + 2, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        c = self.label_emb(labels).view(input.shape[0], 2, 1, 1)\n",
    "        input = torch.cat([input, c], 1)\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unc_model = UnconditionalGenerator(ngpu).to(device)\n",
    "unc_model.load_state_dict(torch.load(\"unc_netG\", map_location=device))\n",
    "\n",
    "cond_model = ConditionalGenerator(ngpu).to(device)\n",
    "cond_model.load_state_dict(torch.load(\"cond_netG\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def binary_classification(dataloader, model, n_tests, is_cond):\n",
    "    results = []\n",
    "    \n",
    "    for _ in range(n_tests): \n",
    "        real, fake, _ = create_images_mixed(dataloader, model, is_cond)\n",
    "        results.append(show_images_mixed(real, fake))\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    correct_guesses = [x == int(y) for x, y in results]\n",
    "    \n",
    "    return correct_guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_tests = 2\n",
    "\n",
    "# results = []\n",
    "\n",
    "# results.append(binary_classification(dataloader, unc_model, n_tests, False))\n",
    "# results.append(binary_classification(dataloader, cond_model, n_tests, True))\n",
    "\n",
    "# print(\"Unconditional:\\t{0}/{2} correct\\nConditional:\\t{1}/{2} correct\".format(sum(results[0]), sum(results[1]), n_tests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = []\n",
    "\n",
    "# imgs, labels = create_fake_images(cond_model, True, n_tests)\n",
    "\n",
    "# for i in range(n_tests):\n",
    "#     results.append(show_labeled_fake_image(imgs[i].to(\"cpu\").detach(), labels[i].item()))\n",
    "#     clear_output(wait=True)\n",
    "    \n",
    "# correct_guesses = sum([ x == int(y) for x, y in results])\n",
    "# undetermined    = sum([-1 == int(y) for _, y in results])\n",
    "\n",
    "# print(\"{}/{} correct with {} undetermined\".format(correct_guesses, n_tests, undetermined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LayerSize(input_size, kernel_size, stride, padding):\n",
    "    return (input_size + 2 * padding - kernel_size) // stride + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.last_filter_dim = 32\n",
    "        \n",
    "        self.conv0 = nn.Conv2d(in_channels=3,  out_channels=8,                    kernel_size=3)\n",
    "        self.conv1 = nn.Conv2d(in_channels=8,  out_channels=16,                   kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=self.last_filter_dim, kernel_size=3)\n",
    "        \n",
    "        self.fc_size = LayerSize(input_size,   3, 1, 0) # Size after conv0\n",
    "        self.fc_size = LayerSize(self.fc_size, 3, 1, 0) # Size after conv1\n",
    "        self.fc_size = LayerSize(self.fc_size, 3, 1, 0) # Size after conv2\n",
    "        self.fc_size = LayerSize(self.fc_size, 2, 2, 0) # Size after max pooling 0\n",
    "        \n",
    "        self.fc0 = nn.Linear(self.fc_size**2 * self.last_filter_dim, 500)\n",
    "        self.fc1 = nn.Linear(500, 2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.BatchNorm2d(F.relu(self.conv0(x)))\n",
    "        x = nn.BatchNorm2d(F.relu(self.conv1(x)))\n",
    "        x = nn.BatchNorm2d(F.relu(self.conv2(x)))\n",
    "        \n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        \n",
    "        x = torch.reshape(x, (-1, self.fc_size**2 * self.last_filter_dim))\n",
    "        x = F.relu(self.fc0(x))\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_real = EvalNet(64)\n",
    "eval_fake = EvalNet(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(dataroot, model, batch_size=128):\n",
    "    real_dataset = ImageFolder(root=dataroot,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.Resize(image_size),\n",
    "                                   transforms.CenterCrop(image_size),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                               ]))\n",
    "    # Create the dataloader\n",
    "    real_dataloader = DataLoader(real_dataset, batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=2)\n",
    "\n",
    "    dataset_size = len(real_dataset)\n",
    "    \n",
    "    # Generate dataset if it has not been generated in a previous run\n",
    "    fake_root = \"generated_\" + dataroot\n",
    "    if not os.path.isdir(fake_root):\n",
    "        cat_path = fake_root + os.path.sep + \"cats\"\n",
    "        dog_path = fake_root + os.path.sep + \"dogs\"\n",
    "        \n",
    "        os.mkdir(fake_root)\n",
    "        os.mkdir(cat_path)\n",
    "        os.mkdir(dog_path)\n",
    "        \n",
    "        instance_amnt = dataset_size // 2\n",
    "        \n",
    "        # Get lists of labels\n",
    "        cat_labels = torch.zeros((instance_amnt,), dtype=torch.long)\n",
    "        dog_labels = torch.ones((instance_amnt,),  dtype=torch.long)\n",
    "        \n",
    "        # Get random noise for the generator\n",
    "        cat_noise = torch.randn(instance_amnt, nz, 1, 1)\n",
    "        dog_noise = torch.randn(instance_amnt, nz, 1, 1)\n",
    "        \n",
    "        # Need to generate the images on the CPU because I get timeouts on the GPU, a problem with Windows\n",
    "        model.to(\"cpu\")\n",
    "        \n",
    "        # Generate pictures as tensors\n",
    "        cat_tensor = model(cat_noise, cat_labels)\n",
    "        dog_tensor = model(cat_noise, cat_labels)\n",
    "        \n",
    "        # Save images to directories\n",
    "        for i in range(instance_amnt):\n",
    "            vutils.save_image(cat_tensor[i], \"{}{}{}.png\".format(cat_path, os.path.sep, i))\n",
    "            \n",
    "        for i in range(instance_amnt):\n",
    "            vutils.save_image(dog_tensor[i], \"{}{}{}.png\".format(cat_path, os.path.sep, i + instance_amnt))\n",
    "    \n",
    "    # Create fake dataset and loader from directories\n",
    "    fake_dataset    = ImageFolder(root=fake_root, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "    fake_dataloader = DataLoader(fake_dataset, batch_size=batch_size,shuffle=True, num_workers=2)\n",
    "    \n",
    "    return real_dataset, real_dataloader, fake_dataset, fake_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r_ds_train, r_dl_train, f_ds_train, f_dl_train = create_datasets(\"train\", cond_model)\n",
    "r_ds_val,   r_dl_val,   f_ds_val,   f_dl_val   = create_datasets(\"val\",   cond_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(val_data_loader, val_dataset, model, loss_fn):\n",
    "    losses = []\n",
    "    n_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for b_x, b_y in val_data_loader:\n",
    "            b_x = b_x.to(device)\n",
    "            b_y = b_y.to(device)\n",
    "            \n",
    "            pred = model(b_x)\n",
    "            loss = loss_fn(pred, b_y)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            hard_preds = pred.argmax(dim=1)\n",
    "            n_correct += torch.sum(pred.argmax(dim=1) == b_y).item()\n",
    "        val_accuracy = n_correct/len(val_dataset)\n",
    "        val_avg_loss = sum(losses)/len(losses)    \n",
    "    \n",
    "    return val_accuracy, val_avg_loss\n",
    "\n",
    "def train_model(model, \n",
    "                loss_fn, \n",
    "                optimizer, \n",
    "                epoch_num, \n",
    "                train_dataset, \n",
    "                train_data_loader, \n",
    "                val_dataset, \n",
    "                val_data_loader, \n",
    "                verboser=False,\n",
    "                batch_print_num=10):\n",
    "    train_losses = []\n",
    "    train_accs   = []\n",
    "    val_losses   = []\n",
    "    val_accs     = []\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        losses = []\n",
    "        n_correct = 0\n",
    "        \n",
    "        batch_counter = 0\n",
    "        batch_n       = 0\n",
    "        batch_correct = 0\n",
    "        batch_loss    = []\n",
    "        \n",
    "        for b_x, b_y in train_data_loader:\n",
    "            batch_counter += 1\n",
    "            \n",
    "            b_x = b_x.to(device)\n",
    "            b_y = b_y.to(device)\n",
    "\n",
    "            # Compute predictions and losses\n",
    "            pred = model(b_x)\n",
    "            loss = loss_fn(pred, b_y)\n",
    "            losses.append(loss.item())\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "            # Count number of correct predictions\n",
    "            hard_preds = pred.argmax(dim=1)\n",
    "            correct    = torch.sum(pred.argmax(dim=1) == b_y).item()\n",
    "            n_correct += correct\n",
    "            \n",
    "            batch_n       += len(b_y)\n",
    "            batch_correct += correct\n",
    "            \n",
    "            if verboser and batch_counter >= batch_print_num:\n",
    "                print(\"Batch loss: {:.3f}\\tBatch acc: {:.2f}\".format(sum(batch_loss)/len(batch_loss), \n",
    "                                                                     batch_correct/batch_n))\n",
    "                batch_counter = 0\n",
    "                batch_n       = 0\n",
    "                batch_correct = 0\n",
    "                batch_loss    = []\n",
    "\n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()    \n",
    "\n",
    "        # Compute accuracy and loss in the entire training set\n",
    "        train_accuracy = n_correct/len(train_dataset)\n",
    "        train_avg_loss = sum(losses)/len(losses)\n",
    "\n",
    "        train_accs.append(train_accuracy)\n",
    "        train_losses.append(train_avg_loss)\n",
    "\n",
    "        # Compute accuracy and loss in the entire validation set\n",
    "        val_accuracy, val_avg_loss = evaluate_model(val_data_loader, val_dataset, model, loss_fn)\n",
    "        val_accs.append(val_accuracy)\n",
    "        val_losses.append(val_avg_loss)\n",
    "\n",
    "        # Display metrics\n",
    "        display_str = 'Epoch {} '\n",
    "        display_str += '\\tLoss: {:.3f} '\n",
    "        display_str += '\\tLoss (val): {:.3f}'\n",
    "        display_str += '\\tAccuracy: {:.2f} '\n",
    "        display_str += '\\tAccuracy (val): {:.2f}'\n",
    "        print(display_str.format(epoch, train_avg_loss, val_avg_loss, train_accuracy, val_accuracy))\n",
    "        \n",
    "    return model, train_accs, train_losses, val_accs, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 15\n",
    "lr        = 0.01\n",
    "\n",
    "loss_fn     = nn.NLLLoss()\n",
    "optimizer_r = optim.Adam(eval_real.parameters(), lr=lr)\n",
    "optimizer_f = optim.Adam(eval_fake.parameters(), lr=lr)\n",
    "    \n",
    "# _, r_ta, r_tl, r_va, r_vl = train_model(eval_real, \n",
    "#                                         loss_fn, \n",
    "#                                         optimizer_r, \n",
    "#                                         epoch_num, \n",
    "#                                         r_ds_train, \n",
    "#                                         r_dl_train, \n",
    "#                                         r_ds_val, \n",
    "#                                         r_dl_val)\n",
    "_, f_ta, f_tl, f_va, f_vl = train_model(eval_fake, \n",
    "                                        loss_fn, \n",
    "                                        optimizer_f, \n",
    "                                        epoch_num, \n",
    "                                        f_ds_train, \n",
    "                                        f_dl_train, \n",
    "                                        f_ds_val, \n",
    "                                        f_dl_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accs_losses(train_accs, train_losses, val_accs, val_losses, epoch_num):\n",
    "    fig, axes = plt.subplots(1,2)\n",
    "\n",
    "    fig.set_size_inches(20, 8)\n",
    "\n",
    "    axes[0].plot(train_losses)\n",
    "    axes[0].plot(val_losses)\n",
    "    axes[0].legend([\"Train\", \"Val\"])\n",
    "    axes[0].set_title(\"Losses\")\n",
    "    axes[0].set_xlabel(\"Epochs\")\n",
    "    axes[0].set_xticks(range(epoch_num))\n",
    "    axes[0].set_xlim([0, epoch_num - 1])\n",
    "    # axes[0].set_ylim([0, max(max(train_losses), max(val_losses))])\n",
    "\n",
    "    axes[1].plot(train_accs)\n",
    "    axes[1].plot(val_accs)\n",
    "    axes[1].legend([\"Train\", \"Val\"])\n",
    "    axes[1].set_title(\"Accuracy\")\n",
    "    axes[1].set_xlabel(\"Epochs\")\n",
    "    axes[1].set_xticks(range(epoch_num))\n",
    "    axes[1].set_xlim([0, epoch_num - 1])\n",
    "    axes[1].set_ylim([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accs_losses(r_ta, r_tl, r_va, r_vl, epoch_num)\n",
    "plot_accs_losses(f_ta, f_tl, f_va, f_vl, epoch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_rd_acc, rm_rd_loss = evaluate_model(r_dl_val, r_ds_val, eval_real, loss_fn)\n",
    "rm_fd_acc, rm_fd_loss = evaluate_model(f_dl_val, f_ds_val, eval_real, loss_fn)\n",
    "fm_rd_acc, fm_rd_loss = evaluate_model(r_dl_val, r_ds_val, eval_fake, loss_fn)\n",
    "fm_fd_acc, fm_fd_loss = evaluate_model(f_dl_val, f_ds_val, eval_fake, loss_fn)\n",
    "\n",
    "print(\"Real model + real data acc: {:.3f}\\taverage loss: {:.3f}\".format(rm_rd_acc, rm_rd_loss))\n",
    "print(\"Real model + fake data acc: {:.3f}\\taverage loss: {:.3f}\".format(rm_fd_acc, rm_fd_loss))\n",
    "print(\"Fake model + real data acc: {:.3f}\\taverage loss: {:.3f}\".format(fm_rd_acc, fm_rd_loss))\n",
    "print(\"Fake model + fake data acc: {:.3f}\\taverage loss: {:.3f}\".format(fm_fd_acc, fm_fd_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
